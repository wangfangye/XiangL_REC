{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCrimal_df = pd.read_csv('allcrime1227.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  edu_level  race  gender  labels              laws  \\\n0    1          3     0       0      13  中华人民共和国刑法第一百三十三条   \n1    2          2     0       1     149  中华人民共和国刑法第三百八十二条   \n2    0          6     0       0      72  中华人民共和国刑法第三百四十七条   \n3    1          7     0       0     189  中华人民共和国刑法第二百六十四条   \n4    2          5     0       0      11  中华人民共和国刑法第三百五十四条   \n\n                                                fact  \n0  2018年4月17日21时1 8分许，被告人崔某某驾驶浙BJ3***号牌小型普通客车沿本市海...  \n1  2011年4月，南京市浦口区盘城街道永锦北路项目征地拆迁过程中，被告人徐金凤作为**街道征地...  \n2  2017年10月至2018年1月期间，被告人张某某从他人处购买甲基苯丙胺片剂（俗称“麻果”）...  \n3  1.2018年2月18日15时许,被告人张某甲、张某乙经预谋，趁成都市锦江区较场坝东街53号...  \n4  1.2018年1月中旬某日15时许，被告人陈某某在自己位于安乡县黄山**的家中容留吸毒人员沈...  \n"
     ]
    }
   ],
   "source": [
    "print(allCrimal_df.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic2 = allCrimal_df[allCrimal_df['labels'].isin([152,13])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12396"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traffic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n13     2739\n152    9657\ndtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic2.groupby(['labels']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "def vec2bow(trafic_vec):\n",
    "    tokenizer = Tokenizer(num_words=2000)\n",
    "    tokenizer.fit_on_texts(trafic_vec)\n",
    "    trafic_vec_seq = tokenizer.texts_to_sequences(trafic_vec)\n",
    "    trafic_fact = sequence.pad_sequences(trafic_vec_seq,maxlen=150)\n",
    "    return trafic_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\wangfy\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 2.042 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "traffic2fact = tokenization(traffic2['fact'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic2vec = vec2bow(traffic2fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traffic2vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(vector1,vector2):\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "simall = np.zeros((len(traffic2vec),len(traffic2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12396, 12396)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "simtest = np.zeros((10,len(traffic2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector0---time0:00:00.373769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1---time0:00:00.543665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector2---time0:00:00.593633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector3---time0:00:00.366775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector4---time0:00:00.378766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector5---time0:00:00.367774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector6---time0:00:00.361777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector7---time0:00:00.361778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector8---time0:00:00.371771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector9---time0:00:00.373769\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "for v1 in range(10):\n",
    "    start = datetime.datetime.now()\n",
    "    for v2 in range(12396):\n",
    "        simtest[v1,v2] = similarity(traffic2vec[v1],traffic2vec[v2])\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"vector{}---time{}\".format(v1,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23466291, 0.24675562, 0.2131649 , 0.23400859, 0.22175356,\n       0.2747693 , 0.27608371, 0.23045448, 0.27446992, 0.27619823])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simtest.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(simtest.shape[1]),simtest[6,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(500),simtest[9,:500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                                          1\nedu_level                                                    3\nrace                                                         0\ngender                                                       0\nlabels                                                      13\nlaws                                          中华人民共和国刑法第一百三十三条\nfact         2018年4月17日21时1 8分许，被告人崔某某驾驶浙BJ3***号牌小型普通客车沿本市海...\nName: 0, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic2.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[('b',2),('a',1),('a',2),('a',3),('c',3),('d',4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('a', 3), ('b', 2), ('c', 3), ('d', 4)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(L,key = lambda k:k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5031026124151314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    " \n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ' '.join(list(s))\n",
    "    \n",
    "    # 将字中间加入空格\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # 转化为TF矩阵\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # 计算TF系数\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    " \n",
    "s1 = '你在干啥子呢'\n",
    "s2 = '你在干什么呢'\n",
    "print(tfidf_similarity(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "def text_cnn(maxlen=150, max_features=2000, embed_size=1):\n",
    "    # Inputs\n",
    "    comment_seq = Input(shape=[maxlen], name='x_seq')\n",
    "    print(type(comment_seq))\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(comment_seq)\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        # l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)show\n",
    "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
    "\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.5)(merge)\n",
    "    output = Dense(32, activation='relu')(out)\n",
    "    output = Dense(units=203, activation='softmax')(output)\n",
    "\n",
    "    model = Model([comment_seq], output)\n",
    "    #     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
